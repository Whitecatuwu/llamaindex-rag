import asyncio
import aiohttp
import sqlite3
from loguru import logger
from pathlib import Path
from typing import List, Dict, Set, Tuple
from urllib.parse import quote
from pathvalidate import sanitize_filename as lib_sanitize
from aiohttp import ClientConnectorError, ServerDisconnectedError, ClientPayloadError

# è¨­å®šæ—¥èªŒ
log_dir = Path("logs")
log_file = log_dir / "crawler_{time}.log"

logger.remove()
logger.add(
    log_file,
    rotation="256 MB",  # æ¯å€‹æª”æ¡ˆæ»¿ 256MB å°±åˆ‡åˆ†
    retention="10 days",  # åªä¿ç•™æœ€è¿‘ 10 å¤©çš„æ—¥èªŒ (è‡ªå‹•åˆªé™¤èˆŠçš„)
    compression="zip",  # åˆ‡åˆ†å¾Œçš„èˆŠæª”æ¡ˆè‡ªå‹•å£“ç¸®æˆ zip (ç¯€çœç©ºé–“)
    encoding="utf-8",  # é˜²æ­¢ä¸­æ–‡äº‚ç¢¼
    level="INFO",  # æª”æ¡ˆä¸­åªå­˜ INFO ä»¥ä¸Š (éæ¿¾æ‰ DEBUG/TRACE)
    enqueue=True,
)

# é…ç½®
BASE_URL = "https://battle-cats.fandom.com/api.php"
DATA_DIR = Path("data/raw/wiki")
HTML_DIR = DATA_DIR / "html"
DB_PATH = DATA_DIR / "wiki_registry.db"

# å»ºç«‹ç›®éŒ„
HTML_DIR.mkdir(parents=True, exist_ok=True)


class WikiCrawler:
    def __init__(self):
        self.conn = sqlite3.connect(DB_PATH)
        self._init_db()
        # é™åˆ¶ä¸¦ç™¼æ•¸ï¼Œé¿å…å° Fandom é€ æˆ DoS
        self.semaphore = asyncio.Semaphore(5)

    def _init_db(self):
        """åˆå§‹åŒ– SQLite ç”¨æ–¼è¿½è¹¤é é¢ç‹€æ…‹"""
        cursor = self.conn.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS pages (
                page_id INTEGER PRIMARY KEY,
                title TEXT UNIQUE,
                last_revid INTEGER,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_path TEXT
            )
        """
        )
        self.conn.commit()

    async def fetch_all_pages_metadata(self, session) -> Dict[str, int]:
        """
        ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿç²å–å…¨ç«™æ‰€æœ‰é é¢çš„ (Title, Revision ID)
        é€™ä¸æœƒä¸‹è¼‰ HTMLï¼ŒåªæŠ“æ¸…å–®ï¼Œé€Ÿåº¦å¾ˆå¿«ã€‚
        """
        logger.info("ğŸ“¡ Fetching global page list and revision IDs...")
        pages_metadata = {}

        params = {
            "action": "query",
            "format": "json",
            "list": "allpages",
            "aplimit": "500",  # ä¸€æ¬¡æ‹¿ 500 ç­†
            "apnamespace": "0",  # 0 = Main Content (æ’é™¤ Talk, User ç­‰)
            "apfilterredir": "nonredirects",  # æ’é™¤é‡å®šå‘é é¢
        }

        while True:
            async with session.get(BASE_URL, params=params) as resp:
                if resp.status != 200:
                    logger.error(f"Error fetching list: {resp.status}")
                    break

                data = await resp.json()

                # é€™è£¡ API åªçµ¦äº† title å’Œ pageidï¼Œç‚ºäº†æ‹¿åˆ° revidï¼Œæˆ‘å€‘é€šå¸¸éœ€è¦
                # åœ¨é€™è£¡å…ˆæ”¶é›† pageidsï¼Œç„¶å¾Œå†ç™¼é€ä¸€æ¬¡ query æŸ¥ revidï¼Œ
                # æˆ–è€…æ”¹ç”¨ generator=allpages & prop=info|revisions (å¦‚ä¸‹å„ªåŒ–)
                pass
                # å‚™è¨»ï¼šæ¨™æº– allpages ä¸ç›´æ¥çµ¦ revidï¼Œç‚ºæ±‚ç²¾ç¢ºèˆ‡æ•ˆç‡ï¼Œ
                # æˆ‘å€‘æ”¹ç”¨ä¸‹é¢çš„é‚è¼¯ (Generator approach)
                break

        # --- å„ªåŒ–ç‰ˆï¼šä½¿ç”¨ Generator ç›´æ¥ç²å– RevID ---
        gen_params = {
            "action": "query",
            "format": "json",
            "generator": "allpages",
            "gaplimit": "50",  # Generator é™åˆ¶è¼ƒåš´ï¼Œä¸€æ¬¡ 50
            "gapnamespace": "0",
            "gapfilterredir": "nonredirects",
            "prop": "info|revisions",  # åŒæ™‚æŠ“å– info å’Œ revision
            "rvprop": "ids",  # åªè¦ revid
        }

        continue_token = {}
        total_fetched = 0

        while True:
            req_params = {**gen_params, **continue_token}
            async with session.get(BASE_URL, params=req_params) as resp:
                data = await resp.json()

                if "query" in data and "pages" in data["query"]:
                    batch = data["query"]["pages"]
                    for pid, info in batch.items():
                        title = info["title"]
                        # å–å¾—æœ€æ–° revid
                        revid = 0
                        if "revisions" in info:
                            revid = info["revisions"][0]["revid"]
                        elif "lastrevid" in info:
                            revid = info["lastrevid"]

                        pages_metadata[title] = revid

                    total_fetched += len(batch)
                    print(f"\râœ… Discovered {total_fetched} pages...", end="")

                # è™•ç†åˆ†é 
                if "continue" in data:
                    continue_token = data["continue"]
                else:
                    break

        print(f"\nâœ¨ Discovery complete. Total pages: {len(pages_metadata)}")
        return pages_metadata

    def get_local_state(self) -> Dict[str, int]:
        """å¾ SQLite è®€å–æœ¬åœ°å·²æœ‰çš„é é¢ç‹€æ…‹"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT title, last_revid FROM pages")
        return {row[0]: row[1] for row in cursor.fetchall()}

    def sanitize_filename(self, title: str) -> str:
        """è™•ç†æª”åä¸­çš„éæ³•å­—å…ƒ"""
        safe_name = lib_sanitize(title, replacement_text="_")

        # ç¢ºä¿ä¸æœƒå› ç‚ºæ›¿æ›å¾Œè®Šæˆç©ºå­—ä¸²
        if not safe_name:
            safe_name = "untitled"

        return f"{safe_name}.html"

    async def fetch_page_html(self, session, title: str, retries: int = 3) -> str:
        """
        ä¸‹è¼‰å–®é  HTML (å«é‡è©¦æ©Ÿåˆ¶)
        :param retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        """
        params = {
            "action": "parse",
            "page": title,
            "format": "json",
            "prop": "text",
            "disablepp": 1,
            "redirects": 1,
        }

        # è¨­å®šè¼ƒå¯¬é¬†çš„è¶…æ™‚ (é€£ç·š 10ç§’ï¼Œè®€å– 30ç§’)
        timeout = aiohttp.ClientTimeout(total=45, connect=10)

        for attempt in range(1, retries + 1):
            try:
                async with session.get(
                    BASE_URL, params=params, timeout=timeout
                ) as resp:
                    # å¦‚æœé‡åˆ° 5xx ä¼ºæœå™¨éŒ¯èª¤ï¼Œä¹Ÿæ‡‰è©²é‡è©¦
                    if resp.status >= 500:
                        logger.warning(
                            f"âš ï¸ Server error {resp.status} for {title}. Attempt {attempt}/{retries}"
                        )
                        raise aiohttp.ClientResponseError(
                            resp.request_info,
                            resp.history,
                            status=resp.status,
                            message="Server Error",
                        )

                    # 404 æˆ–å…¶ä»–éŒ¯èª¤å‰‡ç›´æ¥å›å ±ï¼Œä¸é‡è©¦
                    if resp.status != 200:
                        logger.error(
                            f"âŒ HTTP {resp.status} for {title}: {await resp.text()}"
                        )
                        return None

                    data = await resp.json()
                    if "error" in data:
                        logger.error(f"âŒ API Error for {title}: {data['error']}")
                        return None

                    return data["parse"]["text"]["*"]

            except (
                ClientConnectorError,
                ServerDisconnectedError,
                asyncio.TimeoutError,
                ClientPayloadError,
            ) as e:
                # é€™æ˜¯é æœŸçš„ç¶²è·¯éŒ¯èª¤
                wait_time = 2**attempt  # æŒ‡æ•¸é€€é¿: 2s, 4s, 8s...

                if attempt == retries:
                    logger.error(
                        f"ğŸ’€ Failed to connect for '{title}' after {retries} attempts. Error: {e}"
                    )
                    return None

                logger.warning(
                    f"ğŸ”„ Connection unstable for '{title}' ({e}). Retrying in {wait_time}s..."
                )
                await asyncio.sleep(wait_time)

            except Exception as e:
                # æœªé æœŸçš„éŒ¯èª¤ (å¦‚ JSON è§£æå¤±æ•—)ï¼Œè¨˜éŒ„å¾Œè·³é
                logger.error(f"âŒ Unexpected error for '{title}': {e}")
                return None

    async def process_page(self, session, title: str, remote_revid: int):
        """Worker: ä¸‹è¼‰ -> å­˜æª” -> æ›´æ–° DB"""
        async with self.semaphore:  # é™åˆ¶ä¸¦ç™¼
            try:
                html = await self.fetch_page_html(session, title)
                if not html:
                    return

                # å­˜æª”
                filename = self.sanitize_filename(title)
                file_path = HTML_DIR / filename
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(html)

                # æ›´æ–° DB
                cursor = self.conn.cursor()
                cursor.execute(
                    """
                    INSERT INTO pages (title, last_revid, file_path, last_updated)
                    VALUES (?, ?, ?, CURRENT_TIMESTAMP)
                    ON CONFLICT(title) DO UPDATE SET
                        last_revid = excluded.last_revid,
                        file_path = excluded.file_path,
                        last_updated = CURRENT_TIMESTAMP
                """,
                    (title, remote_revid, str(file_path)),
                )
                self.conn.commit()

                logger.info(f"ğŸ“¥ Updated: {title} (Rev: {remote_revid})")

            except Exception as e:
                logger.error(f"Failed to process {title}: {e}")

    async def run(self):
        # limit=0 è¡¨ç¤ºç„¡ç¸½é™åˆ¶ (ç”± semaphore æ§åˆ¶)ï¼Œlimit_per_host=10 é™åˆ¶å° Fandom çš„é€£ç·š
        # ttl_dns_cache å¯ä»¥æ¸›å°‘ DNS æŸ¥è©¢æ¬¡æ•¸
        connector = aiohttp.TCPConnector(limit=0, limit_per_host=10, ttl_dns_cache=300)
        async with aiohttp.ClientSession(connector=connector) as session:
            # 1. ç²å–é ç«¯æ‰€æœ‰é é¢ç‹€æ…‹
            remote_pages = await self.fetch_all_pages_metadata(session)

            # 2. ç²å–æœ¬åœ°ç‹€æ…‹
            local_pages = self.get_local_state()

            # 3. æ¯”è¼ƒå·®ç•° (Diff)
            tasks = []
            for title, remote_revid in remote_pages.items():
                local_revid = local_pages.get(title)

                # åˆ¤å®šé‚è¼¯ï¼šå¦‚æœæœ¬åœ°æ²’æœ‰ï¼Œæˆ–è€…é ç«¯ç‰ˆæœ¬è¼ƒæ–°
                if local_revid is None or remote_revid > local_revid:
                    # åŠ å…¥ä¸‹è¼‰æ’ç¨‹
                    tasks.append(self.process_page(session, title, remote_revid))

            if not tasks:
                logger.info("ğŸ‰ All pages are up to date!")
                return

            logger.info(f"ğŸš€ Starting download for {len(tasks)} pages...")

            # 4. åŸ·è¡Œä¸‹è¼‰ (ä½¿ç”¨ gather ä¸¦ç™¼)
            # ç‚ºäº†é¿å…ä¸€æ¬¡å¡çˆ†è¨˜æ†¶é«”ï¼Œå¯ä»¥åˆ†æ‰¹è™•ç† (Chunking)
            chunk_size = 50
            for i in range(0, len(tasks), chunk_size):
                chunk = tasks[i : i + chunk_size]
                await asyncio.gather(*chunk)
                logger.info(f"Processing chunk {i}/{len(tasks)}...")
                await asyncio.sleep(1)  # ç¦®è²Œæ€§æš«åœ


if __name__ == "__main__":
    crawler = WikiCrawler()
    asyncio.run(crawler.run())
