import asyncio
import aiohttp
import sqlite3
import json
from loguru import logger
from pathlib import Path
from typing import Any, Coroutine, Dict, Optional
from urllib.parse import quote
from pathvalidate import sanitize_filename as lib_sanitize
from datetime import datetime, timezone
from aiohttp import (
    ClientConnectorError,
    ClientResponseError,
    ServerDisconnectedError,
    ClientPayloadError,
    ContentTypeError,
)


# é…ç½®
BASE_URL = "https://battlecats.miraheze.org/w/api.php"
DATA_DIR = Path("data/raw/wiki")
HTML_DIR = DATA_DIR / "html"
DB_PATH = DATA_DIR / "wiki_registry.db"

# å»ºç«‹ç›®éŒ„
HTML_DIR.mkdir(parents=True, exist_ok=True)


class WikiCrawler:
    def __init__(self):
        self.conn = sqlite3.connect(DB_PATH)
        self._init_db()
        # éµå®ˆ Miraheze API å‹å–„è¦ç¯„
        self.semaphore = asyncio.Semaphore(5)

    def _init_db(self):
        """åˆå§‹åŒ– SQLite ç”¨æ–¼è¿½è¹¤é é¢ç‹€æ…‹"""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS pages (
                page_id INTEGER PRIMARY KEY,
                title TEXT UNIQUE,
                last_revid INTEGER,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_path TEXT,
                categories TEXT
            )
        """)
        self.conn.commit()

    async def fetch_categories(self, session) -> Optional[list]:
        """æŠ“å–é é¢åˆ†é¡"""
        params = {
            "action": "query",
            "prop": "info|recisions",
            "cllimit": "max",
            "gapnamespace":"14",
            "generator":"allpages",
            "gaplimit": "50",
            "format": "json",
            "formatversion": "2"
            ""
        }
        continue_token = {}
        req_params = {**params, **continue_token}

        result = []
        while True:
            fetch_result = await self._fetch(session, req_params)
            if not fetch_result:
                logger.error("Failed to fetch categories list.")
                break

            data, _ = fetch_result
            pages: list = data.get("query", {}).get("pages", [])
            if not pages or "missing" in pages[0]:
                break
            for page in pages:
                result.append(page["title"])
            # ??????
            if "continue" in data:
                continue_token = data["continue"]
                req_params = {**params, **continue_token}
            else:
                break

        return result
    
    async def fetch_all_pages_metadata(self, session) -> Dict[str, int]:
        """
        ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿç²å–å…¨ç«™æ‰€æœ‰é é¢çš„ (Title, Revision ID)
        é€™ä¸æœƒä¸‹è¼‰ HTML, åªæŠ“æ¸…å–®, é€Ÿåº¦å¾ˆå¿«ã€‚
        """
        logger.info("ğŸ“¡ Fetching global page list and revision IDs...")
        pages_metadata = {}

        # --- å„ªåŒ–ç‰ˆï¼šä½¿ç”¨ Generator ç›´æ¥ç²å– RevID ---
        gen_params = {
            "action": "query",
            "format": "json",
            "generator": "allpages",
            "gaplimit": "50",  # Generator é™åˆ¶è¼ƒåš´ï¼Œä¸€æ¬¡ 50
            "gapnamespace": "0",
            "gapfilterredir": "nonredirects",
            "prop": "info|revisions",  # åŒæ™‚æŠ“å– info å’Œ revision
            "rvprop": "ids",  # åªè¦ revid
        }

        continue_token = {}
        total_fetched = 0

        while True:
            req_params = {**gen_params, **continue_token}
            fetch_result = await self._fetch(session, req_params)
            if not fetch_result:
                logger.error("Failed to fetch pages metadata.")
                break

            data, _ = fetch_result

            if "query" in data and "pages" in data["query"]:
                batch = data["query"]["pages"]
                for pid, info in batch.items():
                    title = info["title"]
                    # å–å¾—æœ€æ–° revid
                    revid = 0
                    if "revisions" in info:
                        revid = info["revisions"][0]["revid"]
                    elif "lastrevid" in info:
                        revid = info["lastrevid"]

                    pages_metadata[title] = revid

                total_fetched += len(batch)
                print(f"\r??Discovered {total_fetched} pages...", end="")

            # è™•ç†åˆ†é 
            if "continue" in data:
                continue_token = data["continue"]
            else:
                break
        print(f"\nâœ¨ Discovery complete. Total pages: {len(pages_metadata)}")
        return pages_metadata

    def get_local_state(self) -> Dict[str, int]:
        """å¾ SQLite è®€å–æœ¬åœ°å·²æœ‰çš„é é¢ç‹€æ…‹"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT title, last_revid FROM pages")
        return {row[0]: row[1] for row in cursor.fetchall()}

    def sanitize_filename(self, title: str) -> str:
        """è™•ç†æª”åä¸­çš„éæ³•å­—å…ƒ"""
        safe_name = lib_sanitize(title, replacement_text="_")

        # ç¢ºä¿ä¸æœƒå› ç‚ºæ›¿æ›å¾Œè®Šæˆç©ºå­—ä¸²
        if not safe_name:
            safe_name = "untitled"

        return f"{safe_name}.html"

    async def fetch_page_data(self, session: aiohttp.ClientSession, title: str, retries: int = 3) -> Optional[Dict]:
        """
        ?????? HTML (????????
        :param retries: ??????????
        """
        params = {
            "action": "query",
            "titles": title,
            "prop": "categories|info|revisions",
            "rvprop": "content|ids|timestamp", # ?????? revid
            "rvslots": "*",
            "format": "json",
            "formatversion": "2" # ??? version 2 ?????? list ????????
        }

        fetch_result = await self._fetch(session, params, retries=retries)
        if not fetch_result:
            return None

        data, http_meta = fetch_result

        try:
            # ??? API ???
            if "error" in data:
                logger.error(f"??API Error for {title}: {data['error']}")
                return None
            
            # formatversion=2 ???pages ?????list
            pages = data.get("query", {}).get("pages", [])
            if not pages or "missing" in pages[0]:
                logger.warning(f"??? Page '{title}' not found.")
                return None
            
            page = pages[0]
            revisions = page.get("revisions", [])
            
            # ??? revisions (?????????????????
            if not revisions:
                logger.warning(f"??? No content found for '{title}'")
                return None
            
            revision = revisions[0]
            content = revision.get("slots", {}).get("main", {}).get("content", "")

            # ?????? Canonical URL (?????????)
            # Wiki ?????????????????? URL Encode
            safe_url_title = quote(page.get("title", "").replace(" ", "_"))
            canonical_url = f"https://battlecats.miraheze.org/wiki/{safe_url_title}"

            # ???????JSON
            result = {
                "source": "battlecats.miraheze.org",
                "pageid": page.get("pageid"),
                "title": page.get("title"),
                "canonical_url": canonical_url,  # ??????
                "revid": revision.get("revid"),
                "timestamp": revision.get("timestamp"),
                "content_model": "wikitext",
                "wikitext": content,
                "is_redirect": page.get("redirect", False),
                "redirect_target": None, 
                "fetched_at": datetime.now(timezone.utc).isoformat(),
                "http": http_meta,
            }
            return result
        except Exception as e:
            # ????????? (??JSON ??????)?????????
            logger.error(f"??Unexpected error for '{title}': {e}")
            return None

    async def process_page(self, session, title: str, remote_revid: int):
        """Worker: ä¸‹è¼‰ -> å­˜æª” -> æ›´æ–° DB"""
        async with self.semaphore:  # é™åˆ¶ä¸¦ç™¼
            try:
                page_data = await self.fetch_page_data(session, title)
                if not page_data:
                    return

                # å­˜æª”é‚è¼¯ (ç¢ºä¿å‰¯æª”åæ˜¯ .json)
                # ä½¿ç”¨ rsplit ç¢ºä¿åªæ›¿æ›æœ€å¾Œä¸€å€‹å‰¯æª”åï¼Œé¿å…æª”åä¸­é»è™Ÿèª¤åˆ¤
                safe_name = self.sanitize_filename(title)
                if "." in safe_name:
                    filename = safe_name.rsplit('.', 1)[0] + ".json"
                else:
                    filename = safe_name + ".json"
                
                file_path = HTML_DIR / filename
                
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(page_data, f, ensure_ascii=False, indent=2)

                # æ›´æ–° DB
                cursor = self.conn.cursor()
                cursor.execute("""
                    INSERT INTO pages (page_id, title, last_revid, file_path, last_updated)
                    VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
                    ON CONFLICT(page_id) DO UPDATE SET
                        title = excluded.title, 
                        last_revid = excluded.last_revid,
                        file_path = excluded.file_path,
                        last_updated = CURRENT_TIMESTAMP
                """, (page_data["pageid"], title, remote_revid, str(file_path)))
                self.conn.commit()

                logger.info(f"ğŸ’¾ Saved JSON: {title}")

            except sqlite3.IntegrityError as e:
                # æ•æ‰æ¥µç«¯æƒ…æ³ï¼šå¦‚æœæ–°æ¨™é¡Œè·Ÿã€Œå¦ä¸€ç­†ã€èˆŠè³‡æ–™çš„æ¨™é¡Œè¡çª (Swap Case)
                logger.error(f"âŒ Database Integrity Error for {title}: {e}")
                self.conn.rollback()

            except Exception as e:
                logger.error(f"Failed to process {title}: {e}")

    async def run(self):
        # limit=0 è¡¨ç¤ºç„¡ç¸½é™åˆ¶ (ç”± semaphore æ§åˆ¶)ï¼Œlimit_per_host=10 é™åˆ¶å° Fandom çš„é€£ç·š
        # ttl_dns_cache å¯ä»¥æ¸›å°‘ DNS æŸ¥è©¢æ¬¡æ•¸
        connector = aiohttp.TCPConnector(limit=0, limit_per_host=10, ttl_dns_cache=300)
        async with aiohttp.ClientSession(connector=connector) as session:
            # 1. ç²å–é ç«¯æ‰€æœ‰é é¢ç‹€æ…‹
            remote_pages = await self.fetch_all_pages_metadata(session)

            # 2. ç²å–æœ¬åœ°ç‹€æ…‹
            local_pages = self.get_local_state()

            # 3. æ¯”è¼ƒå·®ç•° (Diff)
            tasks: list[Coroutine[Any, Any, None]] = []
            for title, remote_revid in remote_pages.items():
                local_revid = local_pages.get(title)

                # åˆ¤å®šé‚è¼¯ï¼šå¦‚æœæœ¬åœ°æ²’æœ‰ï¼Œæˆ–è€…é ç«¯ç‰ˆæœ¬è¼ƒæ–°
                if local_revid is None or remote_revid > local_revid:
                    # åŠ å…¥ä¸‹è¼‰æ’ç¨‹
                    tasks.append(self.process_page(session, title, remote_revid))

            if not tasks:
                logger.info("ğŸ‰ All pages are up to date!")
                return

            logger.info(f"ğŸš€ Starting download for {len(tasks)} pages...")

            # 4. åŸ·è¡Œä¸‹è¼‰ (ä½¿ç”¨ gather ä¸¦ç™¼)
            # ç‚ºäº†é¿å…ä¸€æ¬¡å¡çˆ†è¨˜æ†¶é«”ï¼Œå¯ä»¥åˆ†æ‰¹è™•ç† (Chunking)
            chunk_size = 50
            for i in range(0, len(tasks), chunk_size):
                chunk: list[Coroutine[Any, Any, None]] = tasks[i : i + chunk_size]
                await asyncio.gather(*chunk)
                logger.info(f"Processing chunk {i}/{len(tasks)}...")
                await asyncio.sleep(1)  # ç¦®è²Œæ€§æš«åœ

    async def _fetch(
        self,
        session: aiohttp.ClientSession,
        params: Dict[str, Any],
        retries: int = 3,
    ) -> Optional[tuple[Dict[str, Any], Dict[str, Any]]]:
        # è¨­å®šè¼ƒå¯¬é¬†çš„è¶…æ™‚ (é€£ç·š 10ç§’ï¼Œè®€å– 30ç§’)
        timeout = aiohttp.ClientTimeout(total=45, connect=10)

        for attempt in range(1, retries + 1):
            try:
                async with session.get(
                    BASE_URL, params=params, timeout=timeout
                ) as resp:
                    # å¦‚æœé‡åˆ° 5xx / 429 ä¼ºæœå™¨éŒ¯èª¤ï¼Œä¹Ÿæ‡‰è©²é‡è©¦
                    if resp.status >= 500 or resp.status == 429:
                        logger.warning(
                            f"âš ï¸ Server error {resp.status}. Attempt {attempt}/{retries}"
                        )
                        raise aiohttp.ClientResponseError(
                            resp.request_info,
                            resp.history,
                            status=resp.status,
                            message="Server Error",
                        )

                     # 404 æˆ–å…¶ä»–éŒ¯èª¤å‰‡ç›´æ¥å›å ±ï¼Œä¸é‡è©¦
                    if resp.status != 200:
                        logger.error(f"??HTTP {resp.status}: {await resp.text()}")
                        return None

                    data = await resp.json()
                    http_meta = {
                        "status": resp.status,
                        "etag": resp.headers.get("ETag", ""),
                        "last_modified": resp.headers.get("Last-Modified", ""),
                    }
                    return data, http_meta

            except (
                ClientResponseError,
                ClientConnectorError,
                ServerDisconnectedError,
                asyncio.TimeoutError,
                ClientPayloadError,
                ContentTypeError,
                json.JSONDecodeError,
                ValueError,
            ) as e:
                # é€™æ˜¯é æœŸçš„ç¶²è·¯éŒ¯èª¤
                wait_time = 2**attempt  # æŒ‡æ•¸é€€é¿: 2s, 4s, 8s...

                if attempt == retries:
                    logger.error(
                        f"Failed to connect after {retries} attempts. Error: {e}"
                    )
                    return None

                logger.warning(
                    f"ğŸ”„ Connection unstable ({e}). Retrying in {wait_time}s..."
                )
                await asyncio.sleep(wait_time)

            except Exception as e:
                # æœªé æœŸçš„éŒ¯èª¤ (å¦‚ JSON è§£æå¤±æ•—)ï¼Œè¨˜éŒ„å¾Œè·³é
                logger.error(f"Unexpected error while fetching: {e}")
                return None

if __name__ == "__main__":
    # è¨­å®šæ—¥èªŒ
    log_dir = Path("logs")
    log_file = log_dir / "crawler_{time}.log"

    logger.remove()
    logger.add(
        log_file,
        rotation="256 MB",  # æ¯å€‹æª”æ¡ˆæ»¿ 256MB å°±åˆ‡åˆ†
        retention="10 days",  # åªä¿ç•™æœ€è¿‘ 10 å¤©çš„æ—¥èªŒ (è‡ªå‹•åˆªé™¤èˆŠçš„)
        compression="zip",  # åˆ‡åˆ†å¾Œçš„èˆŠæª”æ¡ˆè‡ªå‹•å£“ç¸®æˆ zip (ç¯€çœç©ºé–“)
        encoding="utf-8",  # é˜²æ­¢ä¸­æ–‡äº‚ç¢¼
        level="INFO",  # æª”æ¡ˆä¸­åªå­˜ INFO ä»¥ä¸Š (éæ¿¾æ‰ DEBUG/TRACE)
        enqueue=True,
    )
    crawler = WikiCrawler()
    async def abc():
        connector = aiohttp.TCPConnector(limit=0, limit_per_host=10, ttl_dns_cache=300)
        session = aiohttp.ClientSession(connector=connector)
        result = await crawler.fetch_categories(session)
        print(result)


    asyncio.run(crawler.run())
