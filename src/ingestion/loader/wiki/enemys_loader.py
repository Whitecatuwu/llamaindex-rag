import json
import requests
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from urllib.parse import urljoin
from typing import List, Dict, Optional

# è¨­å®š
ENEMY_RELEASE_ORDER_URL = "https://battle-cats.fandom.com/wiki/Enemy_Release_Order"
BASE_URL = "https://battle-cats.fandom.com"
OUTPUT_DIR = "data/processed"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "enemy_release_order.json")


def fetch_html(url: str) -> str:
    """æŠ“å– HTML ä¸¦è™•ç†åŸºæœ¬çš„ç¶²è·¯éŒ¯èª¤"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": BASE_URL,  # å¢åŠ  Referer å¢åŠ æ“¬çœŸåº¦
    }
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"âŒ Network error: {e}")
        raise


def parse_enemy_table(html: str) -> List[Dict]:
    """è§£æ Enemies åˆ—è¡¨è¡¨æ ¼"""
    soup = BeautifulSoup(html, "lxml")

    # ä½¿ç”¨æ›´å¯¬å®¹çš„é¸æ“‡å™¨ï¼Œå¦‚æœæ‰¾ä¸åˆ° cro_tableï¼Œå˜—è©¦æ‰¾ä¸€èˆ¬çš„ sortable è¡¨æ ¼
    table = soup.select_one("table.cro_table") or soup.select_one("table.article-table")

    if not table:
        raise RuntimeError("âŒ æ‰¾ä¸åˆ°ç›®æ¨™è¡¨æ ¼ (table.cro_table or table.article-table)")

    rows = table.select("tbody > tr")
    data = []

    # ç•¥éè¡¨é ­ (index 0)
    # æ•µäººè¡¨æ ¼å‰å…©è¡Œæ˜¯ç©ºå€¼ï¼Œéœ€è·³é
    for tr in rows[1 + 2 :]:
        tds = tr.find_all("td")

        # è§£æé‚è¼¯
        enemy_id = int(tds[0].get_text(strip=True)) - 2
        enemy_elem = tds[1].find("a")

        if enemy_elem:
            enemy_name = enemy_elem.get_text(strip=True)
            enemy_href = enemy_elem.get("href") or None
            enemy_url = urljoin(BASE_URL, enemy_href) if enemy_href else None
        else:
            enemy_name = tds[1].get_text(strip=True)
            enemy_url = None

        # é€™è£¡æœ‰æ™‚å€™æœƒæœ‰å¤šå€‹ `<br>` æˆ– `<li>`ï¼Œç”¨ separator=" " è®“è®€å–æ›´è‡ªç„¶
        traits = tds[2].get_text(strip=True)
        traits_list = (
            [name.strip() for name in traits.split("/") if name.strip()]
            if traits.upper() != "N/A"
            else []
        )
        first_appearance = tds[3].get_text(" ", strip=True)

        data.append(
            {
                "id": enemy_id,
                "traits": traits_list,
                "enemy_name": enemy_name,
                "enemy_url": enemy_url,
                "first_appearance": first_appearance,
            }
        )

    return data


def save_json(data: List[Dict], filepath: str):
    """å„²å­˜ JSONï¼Œä¸¦è‡ªå‹•å»ºç«‹è·¯å¾‘"""
    # è‡ªå‹•å»ºç«‹çˆ¶ç›®éŒ„
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… Data saved to: {filepath}")


def main():
    print(f"ğŸš€ Starting scrape: {ENEMY_RELEASE_ORDER_URL}")
    html = fetch_html(ENEMY_RELEASE_ORDER_URL)

    try:
        enemy_data = parse_enemy_table(html)
        print(f"âœ… Parsed {len(enemy_data)} enemies.")

        if enemy_data:
            print(f"ğŸ‘€ Example: {enemy_data[0]}")
            save_json(enemy_data, OUTPUT_FILE)
        else:
            print("âš ï¸ No data found.")

    except RuntimeError as e:
        print(e)


if __name__ == "__main__":
    main()
