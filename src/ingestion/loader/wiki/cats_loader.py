import json
import requests
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from typing import List, Dict, Optional

# è¨­å®šå¸¸æ•¸
CAT_RELEASE_ORDER_URL = "https://battle-cats.fandom.com/wiki/Cat_Release_Order"
BASE_URL = "https://battle-cats.fandom.com"
OUTPUT_DIR = "data/processed"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "cat_release_order.json")


def fetch_html(url: str) -> str:
    """æŠ“å– HTML ä¸¦è™•ç†åŸºæœ¬çš„ç¶²è·¯éŒ¯èª¤"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": BASE_URL,  # å¢åŠ  Referer å¢åŠ æ“¬çœŸåº¦
    }
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"âŒ Network error: {e}")
        raise


def parse_cat_table(html: str) -> List[Dict]:
    """è§£æ Battle Cats åˆ—è¡¨è¡¨æ ¼"""
    soup = BeautifulSoup(html, "lxml")

    # ä½¿ç”¨æ›´å¯¬å®¹çš„é¸æ“‡å™¨ï¼Œå¦‚æœæ‰¾ä¸åˆ° cro_tableï¼Œå˜—è©¦æ‰¾ä¸€èˆ¬çš„ sortable è¡¨æ ¼
    table = soup.select_one("table.cro_table") or soup.select_one("table.article-table")

    if not table:
        raise RuntimeError("âŒ æ‰¾ä¸åˆ°ç›®æ¨™è¡¨æ ¼ (table.cro_table or table.article-table)")

    rows = table.select("tbody > tr")
    data = []

    # ç•¥éè¡¨é ­ (index 0)
    for tr in rows[1:]:
        tds = tr.find_all("td")

        # è§£æé‚è¼¯
        cat_id = tds[0].get_text(strip=True) or None
        cat_id = int(cat_id) if cat_id else None
        rarity = tds[1].get_text(strip=True)

        cat_elem = tds[2].find("a")
        if cat_elem:
            cat_name = cat_elem.get_text(strip=True)
            cat_href = cat_elem.get("href") or None
            cat_url = urljoin(BASE_URL, cat_href) if cat_href else None
        else:
            cat_name = tds[2].get_text(strip=True)
            cat_url = None

        # é€™è£¡æœ‰æ™‚å€™æœƒæœ‰å¤šå€‹ `<br>` æˆ– `<li>`ï¼Œç”¨ separator=" " è®“è®€å–æ›´è‡ªç„¶
        evolved = tds[3].get_text(strip=True)
        evolved_list = (
            [name.strip() for name in evolved.split("/") if name.strip()]
            if evolved.upper() != "N/A"
            else []
        )
        obtaining = tds[4].get_text(" ", strip=True)

        data.append(
            {
                "id": cat_id,
                "rarity": rarity,
                "cat_name": cat_name,
                "cat_url": cat_url,
                "evolved_true_ultra": evolved_list,
                "obtaining_method": obtaining,
            }
        )

    return data


def save_json(data: List[Dict], filepath: str):
    """å„²å­˜ JSONï¼Œä¸¦è‡ªå‹•å»ºç«‹è·¯å¾‘"""
    # è‡ªå‹•å»ºç«‹çˆ¶ç›®éŒ„
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… Data saved to: {filepath}")


def main():
    print(f"ğŸš€ Starting scrape: {CAT_RELEASE_ORDER_URL}")
    html = fetch_html(CAT_RELEASE_ORDER_URL)

    try:
        cat_data = parse_cat_table(html)
        print(f"âœ… Parsed {len(cat_data)} cats.")

        if cat_data:
            print(f"ğŸ‘€ Example: {cat_data[0]}")
            save_json(cat_data, OUTPUT_FILE)
        else:
            print("âš ï¸ No data found.")

    except RuntimeError as e:
        print(e)


if __name__ == "__main__":
    main()
